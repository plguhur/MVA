{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for NLP - Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done with Python 3.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import io\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = \"data/\"\n",
    "SST_DIR = os.path.join(PATH_TO_DATA,'SST')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Monolingual (English) word embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2vec():\n",
    "    def __init__(self, fname, nmax=100000):\n",
    "        self.load_wordvec(fname, nmax)\n",
    "        self.word2id = dict.fromkeys(self.word2vec.keys())\n",
    "        self.id2word = {v: k for k, v in self.word2id.items()}\n",
    "        self.embeddings = np.array(self.word2vec.values())\n",
    "    \n",
    "    def load_wordvec(self, fname, nmax):\n",
    "        self.word2vec = {}\n",
    "        with io.open(fname, encoding='utf-8') as f:\n",
    "            next(f)\n",
    "            for i, line in enumerate(f):\n",
    "                word, vec = line.split(' ', 1)\n",
    "                self.word2vec[word] = np.fromstring(vec, sep=' ')\n",
    "                if i == (nmax - 1):\n",
    "                    break\n",
    "        self.words = np.asarray([*self.word2vec.keys()])\n",
    "        self.size = len(self.word2vec[self.words[0]])\n",
    "        print('Loaded %s pretrained word vectors' % (len(self.word2vec)))\n",
    "\n",
    "    def most_similar(self, w, K=5):\n",
    "        # K most similar words: self.score  -  np.argsort\n",
    "        list_score = [self.score(w, w_dico) for w_dico in self.word2vec]\n",
    "        if np.max(list_score) == 0:\n",
    "            return []\n",
    "        \n",
    "        index_best_words = np.argsort(list_score)[::-1][:K]\n",
    "        return self.words[index_best_words].tolist()\n",
    "\n",
    "    def score(self, w1, w2):\n",
    "        # cosine similarity: np.dot  -  np.linalg.norm\n",
    "        if not w1 in self.word2vec or not w2 in self.word2vec:\n",
    "            return 0\n",
    "        emb1 = self.word2vec[w1]\n",
    "        emb2 = self.word2vec[w2]\n",
    "        return emb1.dot(emb2.T)/(np.linalg.norm(emb1)*np.linalg.norm(emb2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 25000 pretrained word vectors\n",
      "\n",
      "\n",
      "A few scores...\n",
      "cat dog 0.671683666279249\n",
      "dog pet 0.6842064029669219\n",
      "dogs cats 0.7074389328052404\n",
      "paris france 0\n",
      "germany berlin 0\n",
      "\n",
      "\n",
      "The most similar words...\n",
      "cat: cat, cats, kitty, kitten, Cat\n",
      "dog: dog, dogs, puppy, Dog, canine\n",
      "dogs: dogs, dog, Dogs, puppies, cats\n",
      "paris: \n",
      "germany: \n"
     ]
    }
   ],
   "source": [
    "w2v = Word2vec(os.path.join(PATH_TO_DATA, 'crawl-300d-200k.vec'), nmax=25000)\n",
    "\n",
    "# You will be evaluated on the output of the following:\n",
    "print(\"\\n\\nA few scores...\")\n",
    "for w1, w2 in zip(('cat', 'dog', 'dogs', 'paris', 'germany'), ('dog', 'pet', 'cats', 'france', 'berlin')):\n",
    "    print(w1, w2, w2v.score(w1, w2))\n",
    "\n",
    "print(\"\\n\\nThe most similar words...\")\n",
    "for w1 in ['cat', 'dog', 'dogs', 'paris', 'germany']:\n",
    "    best_matches = w2v.most_similar(w1)\n",
    "    print(f\"{w1}: {', '.join(best_matches)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoV():\n",
    "    def __init__(self, w2v):\n",
    "        self.w2v = w2v\n",
    "    \n",
    "    def encode(self, sentences, idf=False):\n",
    "        # takes a list of sentences, outputs a numpy array of sentence embeddings\n",
    "        sentemb = []\n",
    "        for sent in sentences:\n",
    "            if idf is False:\n",
    "                # mean of word vectors\n",
    "                embeddings = [self.w2v.word2vec[w] for w in sent if w in w2v.word2vec]\n",
    "                if len(embeddings) == 0: #if any word in the sentence is in our lookup table\n",
    "                    sentemb.append(np.zeros(self.w2v.size))\n",
    "                else:\n",
    "                    sentemb.append(np.mean(embeddings, axis=0))\n",
    "            else:\n",
    "                # idf-weighted mean of word vectors\n",
    "                embeddings = [idf[w]*self.w2v.word2vec[w] for w in sent if w in w2v.word2vec and w in idf]\n",
    "                if len(embeddings) == 0: #if any word in the sentence is in our lookup table\n",
    "                    sentemb.append(np.zeros(self.w2v.size))\n",
    "                else:\n",
    "                    sentemb.append(np.mean(embeddings, axis=0))\n",
    "        return np.vstack(sentemb)\n",
    "\n",
    "    def most_similar(self, s, sentences, idf=False, K=5):\n",
    "        # get most similar sentences and **print** them\n",
    "        list_score = [self.score(s, sentence, idf) for sentence in sentences]\n",
    "        index_best_sent = (np.argsort(list_score)[::-1][:K])\n",
    "        if max(list_score) == 0:\n",
    "            print(f\"Oops... The words are unkwnown in the sentence {s}!\")\n",
    "            return []\n",
    "        \n",
    "        all_sent = []\n",
    "        print(f'Best {K} matches of \"{s}\"')\n",
    "        for i, idx in enumerate(index_best_sent):\n",
    "            print(f'{i}: {sentences[idx]}')\n",
    "            all_sent.append(sentences[idx])\n",
    "        return all_sent\n",
    "    \n",
    "\n",
    "    def score(self, s1, s2, idf=False):\n",
    "        # cosine similarity: use   np.dot  and  np.linalg.norm\n",
    "        emb1, emb2 = self.encode([s1, s2], idf)\n",
    "        if np.linalg.norm(emb1) == 0 or np.linalg.norm(emb2) == 0:\n",
    "             return 0\n",
    "        return emb1.dot(emb2.T)/(np.linalg.norm(emb1)*np.linalg.norm(emb2))\n",
    "        \n",
    "                                           \n",
    "    def build_idf(self, sentences):\n",
    "        # build the idf dictionary: associate each word to its idf value\n",
    "        idf = {}\n",
    "        for sent in sentences:\n",
    "            for w in set(sent):\n",
    "                idf[w] = idf.get(w, 0) + 1        \n",
    "        for w in idf:\n",
    "            idf[w] = max(1, np.log10(len(sentences) / (idf[w])))\n",
    "        return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 25000 pretrained word vectors\n",
      "Best 5 matches of \"1 smiling african american boy . \n",
      "\"\n",
      "0: 1 smiling african american boy . \n",
      "\n",
      "1: an african american male is singing into a microphone . \n",
      "\n",
      "2: an african american man is jumping in the air , while a boy claps . \n",
      "\n",
      "3: a man in brown is helping a man in red climb a rock . \n",
      "\n",
      "4: a smiling woman carrying her baby in a sling . \n",
      "\n",
      "Score without IDF is 0.9564367613713493\n",
      "Best 5 matches of \"1 smiling african american boy . \n",
      "\"\n",
      "0: 1 smiling african american boy . \n",
      "\n",
      "1: an african american man is jumping in the air , while a boy claps . \n",
      "\n",
      "2: an african american male is singing into a microphone . \n",
      "\n",
      "3: a man rock climbing in a forest . \n",
      "\n",
      "4: a man in brown is helping a man in red climb a rock . \n",
      "\n",
      "Score with IDF is 0.9562052331058694\n"
     ]
    }
   ],
   "source": [
    "w2v = Word2vec(os.path.join(PATH_TO_DATA, 'crawl-300d-200k.vec'), nmax=25000)\n",
    "s2v = BoV(w2v)\n",
    "\n",
    "# Load sentences\n",
    "\n",
    "with open(os.path.join(PATH_TO_DATA, 'sentences.txt')) as f:\n",
    "    sentences = f.readlines()\n",
    "\n",
    "# # You will be evaluated on the output of the following:\n",
    "s2v.most_similar('' if not sentences else sentences[10], sentences)  # BoV-mean\n",
    "score = s2v.score('' if not sentences else sentences[7], '' if not sentences else sentences[13])\n",
    "print(f\"Score without IDF is {score}\")\n",
    "\n",
    "# # Build idf scores for each word\n",
    "idf = s2v.build_idf(sentences) \n",
    "s2v.most_similar('' if not sentences else sentences[10], sentences, idf)  # BoV-idf\n",
    "score = s2v.score('' if not sentences else sentences[7], '' if not sentences else sentences[13], idf)\n",
    "print(f\"Score with IDF is {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Multilingual (English-French) word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a bilingual dictionary of size V_a (e.g French-English).\n",
    "\n",
    "Let's define **X** and **Y** the **French** and **English** matrices.\n",
    "\n",
    "They contain the embeddings associated to the words in the bilingual dictionary.\n",
    "\n",
    "We want to find a **mapping W** that will project the source word space (e.g French) to the target word space (e.g English).\n",
    "\n",
    "Procrustes : **W\\* = argmin || W.X - Y ||  s.t  W^T.W = Id**\n",
    "has a closed form solution:\n",
    "**W = U.V^T  where  U.Sig.V^T = SVD(Y.X^T)**\n",
    "\n",
    "In what follows, you are asked to: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50000 pretrained word vectors\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-8576cd473399>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mw2v_eng\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH_TO_DATA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wiki.en.vec'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mw2v_fr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH_TO_DATA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wiki.fr.vec'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-bb390469ac14>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fname, nmax)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mWord2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_wordvec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid2word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-bb390469ac14>\u001b[0m in \u001b[0;36mload_wordvec\u001b[0;34m(self, fname, nmax)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 1 - Download and load 50k first vectors of\n",
    "#     https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.en.vec\n",
    "#     https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.fr.vec\n",
    "\n",
    "# TYPE CODE HERE\n",
    "try:\n",
    "    from urllib import urlretrieve\n",
    "except ImportError:\n",
    "    from urllib.request import urlretrieve\n",
    "\n",
    "files = [\"wiki.en.vec\", \"wiki.fr.vec\"]\n",
    "urls = [\"https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.en.vec\",\n",
    "       \"https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.fr.vec\"]\n",
    "\n",
    "for f, u in zip(files, urls):\n",
    "    if not os.path.isfile(os.path.join(PATH_TO_DATA, f)):\n",
    "        print(\"Downloading\", f)\n",
    "        urlretrieve(u, os.path.join(PATH_TO_DATA, f))\n",
    "\n",
    "w2v_eng = Word2vec(os.path.join(PATH_TO_DATA, 'wiki.en.vec'), nmax=50000)\n",
    "w2v_fr = Word2vec(os.path.join(PATH_TO_DATA, 'wiki.fr.vec'), nmax=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - Get words that appear in both vocabs (= identical character strings)\n",
    "#     Use it to create the matrix X and Y (of aligned embeddings for these words)\n",
    "\n",
    "common = [w for w in w2v_eng.word2vec if w in w2v_fr.word2vec]\n",
    "X = np.vstack([w2v_eng.word2vec[w] for w in common])\n",
    "Y = np.vstack([w2v_fr.word2vec[w] for w in common])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 - Solve the Procrustes using the scipy package and: scipy.linalg.svd() and get the optimal W\n",
    "#     Now W*French_vector is in the same space as English_vector\n",
    "\n",
    "# TYPE CODE HERE\n",
    "X = X[:100]\n",
    "Y = Y[:100]\n",
    "U, s, Vh = scipy.linalg.svd(Y.dot(X.T))\n",
    "W = U.dot(Vh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 - After alignment with W, give examples of English nearest neighbors of some French words (and vice versa)\n",
    "#     You will be evaluated on that part and the code above\n",
    "\n",
    "# TYPE CODE HERE\n",
    "import scipy.linalg as la\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "class BilingualWord2Vec:\n",
    "    #Class that allows to compute and visualize the alignement from one class to another one\n",
    "    def __init__(self, w2v_1,w2v_2):\n",
    "        self.w2v_lang_1 = w2v_1\n",
    "        self.w2v_lang_2 = w2v_2\n",
    "        \n",
    "    def build_alignement(self,max_size):\n",
    "        #Solve the Procrustes probelm from language 1 to language 2 and give the optimal alinement matrix\n",
    "        words_in_both_vocab = [w for w in self.w2v_lang_1.word2vec if w in self.w2v_lang_2.word2vec]\n",
    "        words_in_both_vocab = words_in_both_vocab[:max_size]\n",
    "        X = np.vstack([self.w2v_lang_1.word2vec[w] for w in words_in_both_vocab]).T\n",
    "        Y = np.vstack([self.w2v_lang_2.word2vec[w] for w in words_in_both_vocab]).T\n",
    "        \n",
    "        U, s, Vh = la.svd(Y.dot(X.T))\n",
    "        return U.dot(Vh)\n",
    "    \n",
    "    def mostsimilarother_lang(self,word,W_alignement,K=5,lang_input = 1,lang_output=2):\n",
    "        #word : input word\n",
    "        #W_alignement : alignement from input word to output\n",
    "        #K: number of nearest neightbor\n",
    "        #lang_input: input language\n",
    "        #lang_output: output language (similar words language)\n",
    "        if lang_input == 1: #settings in order to let the choice of input and output language\n",
    "            word2vec_1 = self.w2v_lang_1.word2vec\n",
    "        else:\n",
    "            word2vec_1 = self.w2v_lang_2.word2vec\n",
    "        \n",
    "        if lang_output == 1:  #settings in order to let the choice of input and output language\n",
    "            word2vec_2 = self.w2v_lang_1.word2vec\n",
    "        else:\n",
    "            word2vec_2 = self.w2v_lang_2.word2vec\n",
    "            \n",
    "        query = W_alignement.dot(word2vec_1[word]) if word in word2vec_1 else 0 #projection in output space\n",
    "        keys = word2vec_2.values()\n",
    "        \n",
    "        list_score = [self.score(query,key) for key in keys]\n",
    "        index_best_words = (np.argsort(list_score)[::-1][:K])\n",
    "        \n",
    "        return np.array(word2vec_2.keys())[index_best_words] #return K most similar words\n",
    "\n",
    "    def score(self, emb1, emb2):\n",
    "        # cosine similarity: np.dot  -  np.linalg.norm\n",
    "        if ((np.linalg.norm(emb1)!=0) and ((np.linalg.norm(emb2)!=0))):\n",
    "            return emb1.dot(emb2.T)/(np.linalg.norm(emb1)*np.linalg.norm(emb2))\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def plotsimilarwordother_lang(self,word,W_alignement,pca,K=5,lang_input = 1,lang_output=2):\n",
    "        #This function plots the word given as input in the output space embedding given the alignement matrix\n",
    "        #It is inspire from the TP1\n",
    "        #word : input word\n",
    "        #W_alignement : alignement from input word to output\n",
    "        #K: number of nearest neightbor\n",
    "        #lang_input: input language\n",
    "        #lang_output: output language (similar words language)\n",
    "        if lang_input == 1:  #settings in order to let the choice of input and output language\n",
    "            word2vec_1 = self.w2v_lang_1.word2vec\n",
    "        else:\n",
    "            word2vec_1 = self.w2v_lang_2.word2vec\n",
    "        \n",
    "        if lang_output == 1:  #settings in order to let the choice of input and output language\n",
    "            word2vec_2 = self.w2v_lang_1.word2vec\n",
    "        else:\n",
    "            word2vec_2 = self.w2v_lang_2.word2vec\n",
    "            \n",
    "            \n",
    "        arr = np.empty((0,word2vec_2.values()[0].shape[0]), dtype='f')\n",
    "        word_labels = [word]\n",
    "        # get close words\n",
    "        close_words = self.mostsimilarother_lang(word,W_alignement,K,lang_input,lang_output)\n",
    "        # add the vector for each of the closest words to the array\n",
    "        pos_word = W_alignement.dot(word2vec_1[word]) if word in word2vec_1 else np.zeros(word2vec_2.values()[0].shape)\n",
    "        arr = np.append(arr, np.array([pos_word]), axis=0)\n",
    "        for wrd_score in close_words:\n",
    "            wrd_vector = word2vec_2[wrd_score]\n",
    "            word_labels.append(wrd_score)\n",
    "            arr = np.append(arr, np.array([wrd_vector]), axis=0)\n",
    "\n",
    "        # find tsne coords for 2 dimensions\n",
    "        np.set_printoptions(suppress=True)\n",
    "        Y = pca.transform(arr)\n",
    "\n",
    "        x_coords = Y[:, 0]\n",
    "        y_coords = Y[:, 1]\n",
    "        # display scatter plot\n",
    "        plt.figure(figsize=(8, 6), dpi=80)\n",
    "        plt.scatter(x_coords, y_coords)\n",
    "\n",
    "        for label, x, y in zip(word_labels, x_coords, y_coords):\n",
    "            plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points', fontsize=12)\n",
    "        plt.xlim(x_coords.min()-0.2, x_coords.max()+0.2)\n",
    "        plt.ylim(y_coords.min()-0.2, y_coords.max()+0.2)\n",
    "        plt.title('Nearest neighbors visualization of the word \"%s\"' % word)\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "biw2vec = BilingualWord2Vec(w2v_eng,w2v_fr)\n",
    "W = biw2vec.build_alignement(19000) \n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(np.array(biw2vec.w2v_lang_2.word2vec.values()))\n",
    "biw2vec.plotsimilarwordother_lang('cat',W,pca,K=10) #test with the english word cat\n",
    "biw2vec.plotsimilarwordother_lang('chien',W.T,pca,K=10,lang_input=2,lang_output=1) #test with the french word cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to dive deeper on this subject: https://github.com/facebookresearch/MUSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Sentence classification with BoV and scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - Load train/dev/test of Stanford Sentiment TreeBank (SST)\n",
    "#     (https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf)\n",
    "\n",
    "x_train,y_train = decode_with_labels(os.path.join(SST_DIR, 'stsa.fine.train'))\n",
    "x_dev,y_dev = decode_with_labels(os.path.join(SST_DIR,'stsa.fine.dev'))\n",
    "x_test = decode_without_labels(os.path.join(SST_DIR, 'stsa.fine.test.X'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/wiki.en.vec'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-eeff9d3c7b4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 2 - Encode sentences with the BoV model above\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mw2v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH_TO_DATA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wiki.en.vec'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0ms2v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBoV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0midf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms2v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_idf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-bb390469ac14>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fname, nmax)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mWord2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_wordvec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid2word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-bb390469ac14>\u001b[0m in \u001b[0;36mload_wordvec\u001b[0;34m(self, fname, nmax)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_wordvec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/wiki.en.vec'"
     ]
    }
   ],
   "source": [
    "# 2 - Encode sentences with the BoV model above\n",
    "w2v = Word2vec(os.path.join(PATH_TO_DATA, 'wiki.en.vec'), nmax=50000)\n",
    "s2v = BoV(w2v)\n",
    "\n",
    "idf = s2v.build_idf(x_train)\n",
    "\n",
    "x_train_encode = s2v.encode(x_train)\n",
    "x_dev_encode = s2v.encode(x_dev)\n",
    "x_test_encode = s2v.encode(x_test)\n",
    "\n",
    "x_train_encode_idf = s2v.encode(x_train,idf)\n",
    "x_dev_encode_idf = s2v.encode(x_dev,idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3 - Learn Logistic Regression on top of sentence embeddings using scikit-learn\n",
    "#     (consider tuning the L2 regularization on the dev set)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "lr = LogisticRegression(C= 0.7,max_iter=1000,tol=1e-10)\n",
    "lr.fit(x_train_encode,y_train)\n",
    "prediction_dev = lr.predict(x_dev_encode)\n",
    "prediction_train = lr.predict(x_train_encode)\n",
    "\n",
    "lr_idf = LogisticRegression(C= 0.5,max_iter=1000,tol=1e-10)\n",
    "lr_idf.fit(x_train_encode_idf,y_train)\n",
    "prediction_dev_idf = lr_idf.predict(x_dev_encode_idf)\n",
    "prediction_train_idf = lr_idf.predict(x_train_encode_idf)\n",
    "\n",
    "\n",
    "print('Precision sur le train set', accuracy_score(y_train,prediction_train))\n",
    "print('Precision sur le dev set', accuracy_score(y_dev,prediction_dev))\n",
    "print('Precision sur le train set idf', accuracy_score(y_train,prediction_train_idf))\n",
    "print('Precision sur le dev set idf ', accuracy_score(y_dev,prediction_dev_idf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 4 - Produce 2210 predictions for the test set (in the same order). One line = one prediction (=0,1,2,3,4).\n",
    "#     Attach the output file \"logreg_bov_y_test_sst.txt\" to your deliverable.\n",
    "#     You will be evaluated on the results of the test set.\n",
    "\n",
    "\n",
    "def write_submission(path_output,prediction):\n",
    "    #function that write the submission\n",
    "    lines = '\\n'.join([str(p) for p in prediction])\n",
    "    with open(path_output,'w') as f:\n",
    "        f.writelines(lines)\n",
    "\n",
    "    \n",
    "\n",
    "prediction_test = lr.predict(x_test_encode)\n",
    "write_submission(os.path.join('.',r'logreg_bov_y_test_sst.txt'),prediction_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BONUS!\n",
    "# 5 - Try to improve performance with another classifier\n",
    "#     Attach the output file \"XXX_bov_y_test_sst.txt\" to your deliverable (where XXX = the name of the classifier)\n",
    "\n",
    " classifier)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100)\n",
    "rf_classifier.fit(x_train_encode,y_train) #No extra regularization, bagging is enough in general\n",
    "prediction_dev_rf = rf_classifier.predict(x_dev_encode)\n",
    "prediction_train_rf = rf_classifier.predict(x_train_encode)\n",
    "\n",
    "\n",
    "rf_classifier_idf = RandomForestClassifier(n_estimators=100)\n",
    "rf_classifier_idf.fit(x_train_encode_idf,y_train) #No extra regularization, bagging is enough in general\n",
    "prediction_dev_rf_idf = rf_classifier_idf.predict(x_dev_encode_idf)\n",
    "prediction_train_rf_idf = rf_classifier_idf.predict(x_train_encode_idf)\n",
    "\n",
    "\n",
    "print 'Random Forest Precision sur le train set', accuracy_score(y_train,prediction_train_rf)\n",
    "print 'Random Forest Precision sur le dev set', accuracy_score(y_dev,prediction_dev_rf)\n",
    "\n",
    "print 'Random Forest Precision sur le train set idf', accuracy_score(y_train,prediction_train_rf_idf)\n",
    "print 'Random Forest Precision sur le dev set idf', accuracy_score(y_dev,prediction_dev_rf_idf)\n",
    "\n",
    "\n",
    "xgb_classifier = XGBClassifier(max_depth=3,n_estimators=300,reg_lambda=1) #with early stopping based on val set\n",
    "xgb_classifier.fit(x_train_encode,y_train,verbose=False,early_stopping_rounds=30,eval_metric='mlogloss',eval_set=[(x_train_encode,y_train),(x_dev_encode,y_dev)])\n",
    "# TYPE CODE HERE\n",
    "prediction_dev_xgb = xgb_classifier.predict(x_dev_encode)\n",
    "prediction_train_xgb = xgb_classifier.predict(x_train_encode)\n",
    "\n",
    "xgb_classifier_idf = XGBClassifier(max_depth=3,n_estimators=300,reg_lambda=1) #with early stopping based on val set\n",
    "xgb_classifier_idf.fit(x_train_encode_idf,y_train,verbose=False,early_stopping_rounds=30,eval_metric='mlogloss',eval_set=[(x_train_encode,y_train),(x_dev_encode,y_dev)])\n",
    "# TYPE CODE HERE\n",
    "prediction_dev_xgb_idf = xgb_classifier_idf.predict(x_dev_encode_idf)\n",
    "prediction_train_xgb_idf = xgb_classifier_idf.predict(x_train_encode_idf)\n",
    "\n",
    "\n",
    "print 'XGBoost Precision sur le train set', accuracy_score(y_train,prediction_train_xgb)\n",
    "print 'XGBoost  Precision sur le dev set', accuracy_score(y_dev,prediction_dev_xgb)\n",
    "\n",
    "print 'XGBoost Precision sur le train set idf', accuracy_score(y_train,prediction_train_xgb_idf)\n",
    "print 'XGBoost  Precision sur le dev set idf', accuracy_score(y_dev,prediction_dev_xgb_idf)\n",
    "\n",
    "\n",
    "\n",
    "lgb_classifier = LGBMClassifier(max_depth=3,n_estimators=300,reg_lambda=1) #with early stopping based on val set\n",
    "lgb_classifier.fit(x_train_encode,y_train,verbose=False,early_stopping_rounds=30,eval_metric='multi_logloss',eval_set=[(x_train_encode,y_train),(x_dev_encode,y_dev)])\n",
    "# TYPE CODE HERE\n",
    "prediction_dev_lgbm = lgb_classifier.predict(x_dev_encode)\n",
    "prediction_train_lgbm = lgb_classifier.predict(x_train_encode)\n",
    "\n",
    "lgb_classifier_idf = LGBMClassifier(max_depth=3,n_estimators=300,reg_lambda=1) #with early stopping based on val set\n",
    "lgb_classifier_idf.fit(x_train_encode_idf,y_train,verbose=False,early_stopping_rounds=30,eval_metric='multi_logloss',eval_set=[(x_train_encode,y_train),(x_dev_encode,y_dev)])\n",
    "# TYPE CODE HERE\n",
    "prediction_dev_lgbm_idf = lgb_classifier_idf.predict(x_dev_encode_idf)\n",
    "prediction_train_lgbm_idf = lgb_classifier_idf.predict(x_train_encode_idf)\n",
    "\n",
    "print 'LGBM Precision sur le train set', accuracy_score(y_train,prediction_train_lgbm_idf)\n",
    "print 'LGBM  Precision sur le dev set', accuracy_score(y_dev,prediction_dev_lgbm_idf)\n",
    "\n",
    "print 'LGBM Precision sur le train set idf', accuracy_score(y_train,prediction_train_lgbm_idf)\n",
    "print 'LGBM  Precision sur le dev set idf', accuracy_score(y_dev,prediction_dev_lgbm_idf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Sentence classification with LSTMs in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - Load train/dev/test sets of SST\n",
    "x_train, y_train = decode_with_labels(os.path.join(SST_DIR, 'stsa.fine.train'))\n",
    "x_dev, y_dev = decode_with_labels(os.path.join(SST_DIR,'stsa.fine.dev'))\n",
    "x_test = decode_without_labels(os.path.join(SST_DIR, 'stsa.fine.test.X'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-9d0caa875227>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mY_dev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_dev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"X:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Y:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# 2 - Transform text to integers using keras.preprocessing.text.one_hot function\n",
    "\n",
    "list_words = set([w for sentences in x_train + x_dev + x_test  for w in sentences])\n",
    "n_words = len(list_words)\n",
    "\n",
    "X_train = [one_hot(' '.join(x), n_words) for x in x_train]\n",
    "X_dev = [one_hot(' '.join(x), n_words) for x in x_dev]\n",
    "X_test = [one_hot(' '.join(x), n_words) for x in x_test]\n",
    "\n",
    "Y_train = to_categorical(y_train)\n",
    "Y_dev = to_categorical(y_dev)\n",
    "print(\"Y:\", Y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Padding input data**\n",
    "\n",
    "Models in Keras (and elsewhere) take batches of sentences of the same length as input. It is because Deep Learning framework have been designed to handle well Tensors, which are particularly suited for fast computation on the GPU.\n",
    "\n",
    "Since sentences have different sizes, we \"pad\" them. That is, we add dummy \"padding\" tokens so that they all have the same length.\n",
    "\n",
    "The input to a Keras model thus has this size : (batchsize, maxseqlen) where maxseqlen is the maximum length of a sentence in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (8544, 56)\n"
     ]
    }
   ],
   "source": [
    "# 3 - Pad your sequences using keras.preprocessing.sequence.pad_sequences\n",
    "#     https://keras.io/preprocessing/sequence/\n",
    "max_len = np.max([len(line) for line in x_train + x_dev + x_test])\n",
    "\n",
    "X_train = pad_sequences(X_train, maxlen=max_len)\n",
    "X_dev = pad_sequences(X_dev, maxlen=max_len)\n",
    "X_test = pad_sequences(X_test, maxlen=max_len)\n",
    "print(\"X:\", X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 - Design and train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 - Design your encoder + classifier using keras.layers\n",
    "#     In Keras, Torch and other deep learning framework, we create a \"container\" which is the Sequential() module.\n",
    "#     Then we add components to this contained : the lookuptable, the LSTM, the classifier etc.\n",
    "#     All of these components are contained in the Sequential() and are trained together.\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Activation\n",
    "\n",
    "embed_dim  = 32  # word embedding dimension\n",
    "nhid = 64  # number of hidden units in the LSTM\n",
    "vocab_size = 0  # size of the vocabulary\n",
    "n_classes  = 5\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embed_dim))\n",
    "model.add(LSTM(nhid, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(n_classes, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, None, 32)          0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 64)                24832     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 25,157\n",
      "Trainable params: 25,157\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 5 - Define your loss/optimizer/metrics\n",
    "\n",
    "loss_classif = 'categorical_crossentropy' # find the right loss for multi-class classification\n",
    "optimizer = 'adam' # find the right optimizer\n",
    "metrics_classif = ['accuracy']\n",
    "\n",
    "# Observe how easy (but blackboxed) this is in Keras\n",
    "model.compile(loss=loss_classif,\n",
    "              optimizer=optimizer,\n",
    "              metrics=metrics_classif)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'assign'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-c7c67e51937f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_dev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/src/keras/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1009\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m             \u001b[0mfit_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1011\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1012\u001b[0m         \u001b[0mfit_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/keras/keras/engine/training.py\u001b[0m in \u001b[0;36m_make_train_function\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    507\u001b[0m                     training_updates = self.optimizer.get_updates(\n\u001b[1;32m    508\u001b[0m                         \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_collected_trainable_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m                         loss=self.total_loss)\n\u001b[0m\u001b[1;32m    510\u001b[0m                 updates = (self.updates +\n\u001b[1;32m    511\u001b[0m                            \u001b[0mtraining_updates\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/keras/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/keras/keras/optimizers.py\u001b[0m in \u001b[0;36mget_updates\u001b[0;34m(self, loss, params)\u001b[0m\n\u001b[1;32m    503\u001b[0m                 \u001b[0mp_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlr_t\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mm_t\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_t\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    506\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m             \u001b[0mnew_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp_t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/keras/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(x, new_x)\u001b[0m\n\u001b[1;32m    982\u001b[0m         \u001b[0mThe\u001b[0m \u001b[0mvariable\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mx\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m     \"\"\"\n\u001b[0;32m--> 984\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.7/lib/python3.6/site-packages/tensorflow/python/ops/state_ops.py\u001b[0m in \u001b[0;36massign\u001b[0;34m(ref, value, validate_shape, use_locking, name)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_locking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_locking\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         validate_shape=validate_shape)\n\u001b[0;32m--> 222\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'assign'"
     ]
    }
   ],
   "source": [
    "# 6 - Train your model and find the best hyperparameters for your dev set\n",
    "#     you will be evaluated on the quality of your predictions on the test set\n",
    "\n",
    "bs = 64\n",
    "n_epochs = 6\n",
    "history = model.fit(X_train, Y_train, batch_size=bs, epochs=n_epochs, validation_data=(X_dev, Y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 7 - Generate your predictions on the test set using model.predict(x_test)\n",
    "#     https://keras.io/models/model/\n",
    "#     Log your predictions in a file (one line = one integer: 0,1,2,3,4)\n",
    "#     Attach the output file \"logreg_lstm_y_test_sst.txt\" to your deliverable.\n",
    "\n",
    "# TYPE CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 -- innovate !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 8 - Open question: find a model that is better on your dev set\n",
    "#     (e.g: use a 1D ConvNet, use a better classifier, pretrain your lookup tables ..)\n",
    "#     you will get point if the results on the test set are better: be careful of not overfitting your dev set too much..\n",
    "#     Attach the output file \"XXX_XXX_y_test_sst.txt\" to your deliverable.\n",
    "\n",
    "# TYPE CODE HERE\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
