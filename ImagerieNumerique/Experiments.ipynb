{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook offers some experiments on the paper:\n",
    "\n",
    "\n",
    "\"Globally and Locally Consistent Image Completion\", SATOSHI IIZUKA, EDGAR SIMO-SERRA,\n",
    "HIROSHI ISHIKAWA\n",
    "\n",
    "The employed code comes from https://github.com/akmtn/pytorch-siggraph2017-inpainting\n",
    "\n",
    "It requires a PyTorch version below 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.models import _NetCompletion, _NetContext, completionnet_places2\n",
    "from src.ablation import completionnet_ablation, copy_weights\n",
    "from src.masking import run_draw\n",
    "from src.inpaint import inpainting, inpainting2, load_network, random_mask\n",
    "from src.inpaint import load_mask, load_data, post_processing\n",
    "from src.train import load_dataset, train_random_mask, train_discriminator, get_networks\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.nn.modules.loss import BCELoss, MSELoss\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.utils as vutils\n",
    "import os\n",
    "from moviepy.editor import *\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import PIL\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "def numpy2torch(im):\n",
    "    im = im.transpose((2,0,1))\n",
    "    return torch.from_numpy(im).float()\n",
    "\n",
    "\n",
    "filename = \"completionnet_places2.t7\"\n",
    "url = \"http://hi.cs.waseda.ac.jp/~iizuka/data/completionnet_places2.t7\"\n",
    "\n",
    "if not os.path.isfile(filename):\n",
    "    urllib.request.urlretrieve(url, filename)\n",
    "    \n",
    "model, datamean = load_network()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple tests on an image\n",
    "Code from https://stackoverflow.com/a/36382158/4986615\n",
    "\n",
    "Press ESC to quit the windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"images/prefilled2-staline.png\")\n",
    "run_draw( img, \"mask.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "mask = cv2.imread(\"mask.png\")\n",
    "plt.imshow(mask)\n",
    "plt.axis('off')\n",
    "plt.title(\"Mask\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "M = load_mask(\"mask.png\", output_shape=(600, 400))\n",
    "I = load_data(\"images/prefilled2-staline.png\", output_shape=(600, 400))\n",
    "out = inpainting(model, datamean, I, M, postproc=False, skip=False)\n",
    "vutils.save_image(out, 'out.png', normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "out_im = cv2.imread(\"out.png\")[:,:,::-1] \n",
    "plt.imshow(out_im)\n",
    "plt.axis('on')\n",
    "plt.title(\"Output\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing on a short video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Next lines are for downloading the required video from Youtube\n",
    "if not os.path.exists(\"wwf_forest.mp4\"):\n",
    "    os.system(\"youtube-dl gpzuVt_mkKs -o wwf_forest.mp4\")\n",
    "\n",
    "clip = VideoFileClip(\"wwf_forest.mp4\").subclip((0,30.0),(0,0.0))\n",
    "w, h = 600, 400\n",
    "clip = clip.resize( (w, h) )\n",
    "clip.ipython_display(fps=20, loop=True, autoplay=True)\n",
    "M = torch.FloatTensor(1, h, w).fill_(0.)\n",
    "mask_w, mask_h = np.random.randint(60,100, 2)\n",
    "px = np.random.randint(0, w-mask_w)\n",
    "py = np.random.randint(0, h-mask_h)\n",
    "M[:, py:py+mask_h, px:px+mask_w] = 1.\n",
    "\n",
    "\n",
    "def inpainting_video(clip, model, datamean, M):\n",
    "    \n",
    "    def fl(gf,t):\n",
    "        im = gf(t)\n",
    "        h,w,d = im.shape\n",
    "        im = im.transpose((2,0,1)).astype(np.float64)\n",
    "        I = torch.from_numpy(im/255.).float()\n",
    "        out = inpainting(model, datamean, I, M, postproc=False, skip=False).data.numpy()\n",
    "        out = (out*255.).transpose((1,2,0)).astype(int)\n",
    "        return out\n",
    "    \n",
    "    return clip.fl(fl)\n",
    "\n",
    "clip_inpainted = clip.fx(inpainting_video, model, datamean, M)\n",
    "# clip_inpainted.ipython_display(fps=20, loop=True, autoplay=True)\n",
    "clip_inpainted.write_videofile('inpainted_30s.mp4', bitrate=\"3000k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VideoFileClip(\"wwf_forest.mp4\").save_frame(\"wood.jpg\", t=26.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computation of the loss function on a set of random masks\n",
    "Here we want to draw a metric for the quality of the reconstruction.\n",
    "We use a sum of a weighted MSE and a binary cross entropy as in the reference paper for training the generator.\n",
    "\n",
    "A metric does not make so much sense for the loss function, because we don't have the reconstruction to be exactly as before but to be consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wtl2 = 0.5\n",
    "bce_loss = BCELoss()\n",
    "mse_loss = MSELoss()\n",
    "        \n",
    "M = random_mask(output_shape=(600, 400))\n",
    "I = load_data(\"images/bridge.jpg\", output_shape=(600, 400))\n",
    "out = inpainting(model, datamean, I, M, postproc=False)\n",
    "out2 = out.float()      \n",
    "\n",
    "error = wtl2*mse_loss(out2, I) + (1 - wtl2)*bce_loss(out2, I)\n",
    "print(\"Normal:\", error)\n",
    "\n",
    "out_proc = post_processing(I, M, out)\n",
    "out_proc2 = out_proc.float()      \n",
    "error = wtl2*mse_loss(out_proc2, I) + (1 - wtl2)*bce_loss(out_proc2, I)\n",
    "print(\"Post-processing:\", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Influence of the hole size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = [20,30,50,80,130,210]\n",
    "N = len(size)\n",
    "w, h = 400, 400\n",
    "masks = torch.FloatTensor(N, h, w).fill_(0.)\n",
    "px = np.random.randint(110, 290)\n",
    "py = np.random.randint(110, 490)\n",
    "image = \"celeb\"\n",
    "I = load_data(f\"images/{image}.jpg\", output_shape=(w, h))\n",
    "\n",
    "for i in range(N):\n",
    "    half = int(size[i]/2)\n",
    "    res_dir = os.path.join(\"results\", \"hole\", image)\n",
    "    os.makedirs(res_dir, exist_ok=True)\n",
    "    masks[i, py-half:py+half, px-half:px+half] = 1.\n",
    "    out = inpainting(model, datamean, I, masks[i:i+1], postproc=False)\n",
    "    out = out.data.numpy()\n",
    "    out = out.transpose((1,2,0))\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.imshow(out)\n",
    "    plt.axis('off')\n",
    "    plt.savefig(os.path.join(res_dir, f'hole-{i}.png'), bbox_inches='tight')\n",
    "    \n",
    "    out = inpainting(model, datamean, I, masks[i:i+1], postproc=False, skip=True)\n",
    "    out = out.data.numpy()\n",
    "    out = out.transpose((1,2,0))\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.imshow(out)\n",
    "    plt.axis('off')\n",
    "    plt.savefig(os.path.join(res_dir, f'hole-sanity-{i}.png'), bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Influence of the hole size inside a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "W, H = 600, 400\n",
    "image = \"forest\"\n",
    "filename = f\"images/{image}.jpg\"\n",
    "input = Image.open(filename)\n",
    "input = input.resize((W, H))\n",
    "clip = ImageClip(np.asarray(input)).subclip(0,5)\n",
    "px = int(W/2)\n",
    "py = int(H/2)\n",
    "M = torch.FloatTensor(1, H, W).fill_(0.)\n",
    "\n",
    "\n",
    "def inpainting_hole(clip, model, datamean, M):\n",
    "    \n",
    "    def fl(gf,t):\n",
    "        im = gf(t)\n",
    "        h,w,d = im.shape\n",
    "        half = int(t*20)\n",
    "        M[:, py-half:py+half, px-half:px+half] = 1.\n",
    "        im = im.transpose((2,0,1)).astype(np.float64)\n",
    "        I = torch.from_numpy(im/255.).float()\n",
    "        out = inpainting(model, datamean, I, M, postproc=False, skip=False).data.numpy()\n",
    "        out = (out*255.).transpose((1,2,0)).astype(int)\n",
    "        return out\n",
    "    \n",
    "    return clip.fl(fl)\n",
    "\n",
    "\n",
    "clip_inpainted = clip.fx(inpainting_hole, model, datamean, M)\n",
    "# clip_inpainted.ipython_display(fps=20, loop=True, autoplay=True)\n",
    "clip_inpainted.write_videofile(f'videos/holen-{image}.mp4', fps=15, codec='mpeg4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Influence of global context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "size = [30,50,70,90,110,130]\n",
    "N = len(size)\n",
    "w, h = 600, 400\n",
    "half = 30\n",
    "image = \"flower\"\n",
    "input = Image.open(f\"images/{image}.jpg\")\n",
    "input.load()\n",
    "W, H = input.size\n",
    "px = int(W/2)\n",
    "py = int(H/2)\n",
    "\n",
    "for i in range(N):\n",
    "    res_dir = os.path.join(\"results\", \"global_context\", image)\n",
    "    os.makedirs(res_dir, exist_ok=True)\n",
    "    \n",
    "    left = max(0, px-half-size[i])\n",
    "    right = min(px + half + size[i], W)\n",
    "    top = max(0, py - half - size[i])\n",
    "    bottom = min(W, py + half + size[i])\n",
    "    I = input.crop((left, top, right, bottom))\n",
    "    M = input.copy()\n",
    "    M.paste((0,0,0), [0,0, *input.size])\n",
    "    M.paste((255,255,255), [px-half, py-half, px+half, py+half])\n",
    "    M = M.crop((left, top, right, bottom))\n",
    "    I = I.resize((w, h))\n",
    "    I = np.asarray(I).astype(float)/255.0\n",
    "    I = I.transpose((2,0,1))\n",
    "    I = torch.from_numpy(I).float()\n",
    "    M = M.resize((w, h))\n",
    "    M = np.asarray(M).astype(float)/255.0\n",
    "    M[M<0.1] = 0.\n",
    "    M = M.transpose((2,0,1))[:1,:,:]\n",
    "    M = torch.from_numpy(M).float()\n",
    "    \n",
    "    out = inpainting(model, datamean, I, M, postproc=False, skip=False)\n",
    "    out = out.data.numpy()\n",
    "    out = out.transpose((1,2,0))\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.imshow(out)\n",
    "    plt.axis('off')\n",
    "    plt.savefig(os.path.join(res_dir, f'hole-{i}.png'), bbox_inches='tight')\n",
    "    \n",
    "    out = inpainting(model, datamean, I, M, postproc=False, skip=True)\n",
    "    out = out.data.numpy()\n",
    "    out = out.transpose((1,2,0))\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.imshow(out)\n",
    "    plt.axis('off')\n",
    "    plt.savefig(os.path.join(res_dir, f'hole-sanity-{i}.png'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Influence of local context\n",
    "We modify the local context on a side, two sides, ..., all sides of the mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "width = 3\n",
    "N = 5\n",
    "w, h = 600, 400\n",
    "half = 60\n",
    "image = \"forest\"\n",
    "input = Image.open(f\"images/{image}.jpg\")\n",
    "input.load()\n",
    "W, H = input.size\n",
    "px = int(W/2)\n",
    "py = int(H/2)\n",
    "x1, y1, x2, y2 = px-half-width, py-half-width, px+half+width, py+half+width\n",
    "points = (x1, y1), (x2, y1), (x2, y2), (x1, y2), (x1, y1)\n",
    "    \n",
    "for i in range(N):\n",
    "    res_dir = os.path.join(\"results\", \"local_context\", image)\n",
    "    os.makedirs(res_dir, exist_ok=True)\n",
    "    \n",
    "    I = input.copy()\n",
    "    \n",
    "    drawing = ImageDraw.Draw(I)\n",
    "    drawing.line(points[:i+1], fill=(255,0,0), width=width)\n",
    "    M = input.copy()\n",
    "    M.paste((0,0,0), [0,0, *input.size])\n",
    "    M.paste((255,255,255), [px-half, py-half, px+half, py+half])\n",
    "    I = I.resize((w, h))\n",
    "    I = np.asarray(I).astype(float)/255.0\n",
    "    I = I.transpose((2,0,1))\n",
    "    I = torch.from_numpy(I).float()\n",
    "    M = M.resize((w, h))\n",
    "    M = np.asarray(M).astype(float)/255.0\n",
    "    M[M<0.1] = 0.\n",
    "    M[M>0.9] = 1.\n",
    "    M = M.transpose((2,0,1))[:1,:,:]\n",
    "    M = torch.from_numpy(M).float()\n",
    "    \n",
    "    out = inpainting(model, datamean, I, M, postproc=False, skip=False)\n",
    "    out = out.data.numpy()\n",
    "    out = out.transpose((1,2,0))\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.imshow(out)\n",
    "    plt.axis('off')\n",
    "    plt.savefig(os.path.join(res_dir, f'hole-{i}.png'), bbox_inches='tight')\n",
    "    \n",
    "    out = inpainting(model, datamean, I, M, postproc=False, skip=True)\n",
    "    out = out.data.numpy()\n",
    "    out = out.transpose((1,2,0))\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.imshow(out)\n",
    "    plt.axis('off')\n",
    "    plt.savefig(os.path.join(res_dir, f'hole-sanity-{i}.png'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limit of pixels taken into account\n",
    "Theoretically, the module is not supposed to use pixels outside a region of $307\\times307$ pixels. Let's check that!\n",
    "We need an high resolution image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# images = [\"flower\", \"bridge\", \"celeb\", \"forest\"]\n",
    "# inputs = [Image.open(f\"images/{image}.jpg\") for image in images]\n",
    "images = [\"flower\", \"forest\"]\n",
    "inputs = [Image.open(f\"images/{image}.jpg\") for image in images]\n",
    "sizes = [[1200, 600], [1200, 600]]\n",
    "N = len(sizes)\n",
    "half = 100\n",
    "crop = 307\n",
    "res_dir = os.path.join(\"results\", \"limit_context\")\n",
    "os.makedirs(res_dir, exist_ok=True)\n",
    "\n",
    "def crop_image(input, crop, half, output_shape, is_mask=False):\n",
    "    W, H = input.size\n",
    "    px = int(W/2)\n",
    "    py = int(H/2)\n",
    "    im = input.copy()\n",
    "    im.paste((0,0,0), [0,0, *input.size])\n",
    "    \n",
    "    if is_mask:\n",
    "        im.paste((255,255,255), [px-half, py-half, px+half, py+half])\n",
    "    else:\n",
    "        points = [px-crop-half, py-half-crop, px+half+crop, py+half+crop]\n",
    "        cropped = input.crop(points)\n",
    "        im.paste(cropped, points)\n",
    "    im = im.resize(output_shape)\n",
    "    im = np.asarray(im).astype(float)/255.0\n",
    "    if is_mask:\n",
    "        im[im<0.1] = 0.\n",
    "    return im\n",
    "    \n",
    "\n",
    "    \n",
    "for i in range(N):\n",
    "    w, h = sizes[i]\n",
    "    input = inputs[i]\n",
    "    image = images[i]\n",
    "    \n",
    "    I = crop_image(input, crop, half, (w, h), is_mask=False)\n",
    "    I = numpy2torch(I)\n",
    "    M = crop_image(input, crop, half, (w, h), is_mask=True)\n",
    "    M = numpy2torch(M[:,:,:1])\n",
    "    I2 = crop_image(input, 1000, half, (w, h), is_mask=False)\n",
    "    I2 = numpy2torch(I2)\n",
    "    M2 = crop_image(input, 1000, half, (w, h), is_mask=True)\n",
    "    M2 = numpy2torch(M2[:,:,:1])\n",
    "    \n",
    "    # cropped inpainting \n",
    "    out = inpainting(model, datamean, I, M, postproc=False, skip=False)\n",
    "    out = out.data.numpy()\n",
    "    out = out.transpose((1,2,0))\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.imshow(out)\n",
    "    plt.axis('off')\n",
    "    plt.savefig(os.path.join(res_dir, f'{image}-crop-{i}.png'), bbox_inches='tight')\n",
    "\n",
    "    # uncropped inpaiting \n",
    "    out = inpainting(model, datamean, I2, M2, postproc=False, skip=False)\n",
    "    out = out.data.numpy()\n",
    "    out = out.transpose((1,2,0))\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.imshow(out)\n",
    "    plt.axis('off')\n",
    "    plt.savefig(os.path.join(res_dir, f'{image}-normal-{i}.png'), bbox_inches='tight')\n",
    "\n",
    "    # sanity check    \n",
    "    out = inpainting(model, datamean, I, M, postproc=False, skip=True)\n",
    "    out = out.data.numpy()\n",
    "    out = out.transpose((1,2,0))\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.imshow(out)\n",
    "    plt.axis('off')\n",
    "    plt.savefig(os.path.join(res_dir, f'{image}-sanity-{i}.png'), bbox_inches='tight')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenate similar images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "W, H = 600, 400\n",
    "image = \"flower\"\n",
    "filename = f\"images/{image}.jpg\"\n",
    "res_dir = os.path.join(\"results\", \"concatenate\", image)\n",
    "os.makedirs(res_dir, exist_ok=True)\n",
    "input = Image.open(filename)\n",
    "input = input.resize((W, H))\n",
    "input = np.asarray(input)\n",
    "I = input.transpose((2,0,1)).astype(float)\n",
    "I = torch.from_numpy(I/255.).float()\n",
    "px = int(W/2)\n",
    "py = int(H/2)\n",
    "M = torch.FloatTensor(1, H, W).fill_(0.)\n",
    "half_y = 150\n",
    "half_w = 150\n",
    "\n",
    "# failing case: we can see the image is becoming more and more nothing\n",
    "M[:, py-half_y:py+half_y, px-half_w:px+half_w] = 1.\n",
    "out = inpainting(model, datamean, I, M, postproc=False, skip=False).data.numpy()\n",
    "out = (out*255.).transpose((1,2,0)).astype(int)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(out)\n",
    "plt.axis('off')\n",
    "plt.savefig(os.path.join(res_dir, f'failing.jpg'), bbox_inches='tight')\n",
    "\n",
    "# sanity check of the failure case\n",
    "out = inpainting(model, datamean, I, M, postproc=False, skip=True).data.numpy()\n",
    "out = (out*255.).transpose((1,2,0)).astype(int)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(out)\n",
    "plt.axis('off')\n",
    "plt.savefig(os.path.join(res_dir, f'sanity-check.jpg'), bbox_inches='tight')\n",
    "\n",
    "# so we concatenate other image around it\n",
    "images = [Image.open(f\"images/{image}{i}.jpg\").resize((W, H)) for i in range(1, 10)]\n",
    "concat = Image.new('RGB', (3*W, 3*H))\n",
    "limit = 309\n",
    "for i, im in enumerate(images):\n",
    "    x_offset = W * (i % 3)\n",
    "    y_offset = H * (i // 3)\n",
    "    concat.paste(im, (x_offset, y_offset))\n",
    "I = np.asarray(concat.resize((W,H))).transpose((2,0,1)).astype(float)/255.\n",
    "I = torch.from_numpy(I).float()\n",
    "\n",
    "M = concat.copy()\n",
    "M.paste((0,0,0), [0,0,3*W,3*H])\n",
    "py, px = int(3*H/2), int(3*W/2)\n",
    "M.paste((255,255,255), [px-half_w, py-half_y, px+half_w, py+half_y])\n",
    "M = np.asarray(M.resize((W,H))).transpose((2,0,1)).astype(float)/255.\n",
    "M = torch.from_numpy(M[:1,:,:]).float()\n",
    "out = inpainting(model, datamean, I, M, postproc=False, skip=False).data.numpy()\n",
    "out = (out*255.).transpose((1,2,0)).astype(int)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(out)\n",
    "plt.axis('off')\n",
    "plt.savefig(os.path.join(res_dir, f'concatenate.jpg'), bbox_inches='tight')\n",
    "\n",
    "# then we deconcatenate:\n",
    "restored = Image.fromarray(out.astype(np.uint8)).resize((3*W, 3*H)) \\\n",
    "            .crop((W, H, 2*W, 2*H))\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(restored)\n",
    "plt.axis('off')\n",
    "plt.savefig(os.path.join(res_dir, f'deconcat.jpg'), bbox_inches='tight')\n",
    "\n",
    "\n",
    "# plt.imshow((I.data.numpy()*255.).transpose((1,2,0)).astype(int))\n",
    "# plt.show()\n",
    "# plt.imshow((M.data.numpy()*255.).transpose((1,2,0)).astype(int)[:,:,0])\n",
    "# plt.show()\n",
    "# plt.imshow(concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenate images within the same video sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def crop_image2(input, crop, output_shape, masking=0):\n",
    "    W, H = input.size\n",
    "    px = int(W/2)\n",
    "    py = int(H/2)\n",
    "    im = input.copy()\n",
    "    if masking > 0:\n",
    "        im.paste((0,0,0), [0,0, *im.size])\n",
    "        im.paste((255,255,255), [px-masking, py-masking, px+masking, py+masking])\n",
    "    im = im.crop([px-crop, py-crop, px+crop, py+crop])\n",
    "    im = im.resize(output_shape)\n",
    "    return im\n",
    "\n",
    "clip = VideoFileClip(\"wwf_forest.mp4\")\n",
    "W, H = 300, 200\n",
    "image = \"video_extract\"\n",
    "# times = np.arange(23.0,26.7, 0.4)\n",
    "times = np.arange(55.0,56., 0.1)\n",
    "orig_images = [Image.fromarray(clip.get_frame(t)) for t in times] \n",
    "del clip\n",
    "crop, size = 120, 80\n",
    "images = [crop_image2(im, crop, (W, H)) for im in orig_images]\n",
    "res_dir = os.path.join(\"results\", \"concatenate\", image)\n",
    "os.makedirs(res_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# failing case: we can see the image is becoming more and more nothing\n",
    "I = images[4]\n",
    "I = torch.from_numpy(np.asarray(I).transpose((2,0,1)).astype(float)/255.0).float()\n",
    "M = crop_image2(orig_images[4], crop, (W, H), size)\n",
    "M = torch.from_numpy(np.asarray(M).transpose((2,0,1))[:1,:,:].astype(float)/255.0).float()\n",
    "# M[M<0.1]=0.\n",
    "out = inpainting(model, datamean, I, M, postproc=False, skip=False).data.numpy()\n",
    "out = (out*255.).transpose((1,2,0)).astype(int)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(out)\n",
    "plt.axis('off')\n",
    "plt.savefig(os.path.join(res_dir, f'failing.jpg'), bbox_inches='tight')\n",
    "\n",
    "# sanity check of the failure case\n",
    "out = inpainting(model, datamean, I, M, postproc=False, skip=True).data.numpy()\n",
    "out = (out*255.).transpose((1,2,0)).astype(int)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(out)\n",
    "plt.axis('off')\n",
    "plt.savefig(os.path.join(res_dir, f'sanity-check.jpg'), bbox_inches='tight')\n",
    "\n",
    "# so we concatenate other image around it\n",
    "concat = Image.new('RGB', (3*W, 3*H))\n",
    "ops = [Image.ROTATE_180, Image.FLIP_TOP_BOTTOM, Image.ROTATE_180, \\\n",
    "       Image.FLIP_LEFT_RIGHT, -1, Image.FLIP_LEFT_RIGHT, \\\n",
    "       Image.ROTATE_180, Image.FLIP_TOP_BOTTOM, Image.ROTATE_180]\n",
    "for i in range(9):\n",
    "    im = images[i]\n",
    "    x_offset = W * (i % 3)\n",
    "    y_offset = H * (i // 3)\n",
    "    if i != 4:\n",
    "        im = im.transpose(ops[i])\n",
    "    concat.paste(im, (x_offset, y_offset))\n",
    "I = torch.from_numpy(np.asarray(concat).transpose((2,0,1)).astype(float)/255.0).float()\n",
    "M = concat.copy()\n",
    "M.paste((0,0,0), [0,0,3*W,3*H])\n",
    "py, px = int(3*H/2), int(3*W/2)\n",
    "M.paste((255,255,255), [px-size, py-size, px+size, py+size])\n",
    "M = torch.from_numpy(np.asarray(M).transpose((2,0,1))[:1,:,:].astype(float)/255.0).float()\n",
    "out = inpainting(model, datamean, I, M, postproc=False, skip=False).data.numpy()\n",
    "out = (out*255.).transpose((1,2,0)).astype(int)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(out)\n",
    "plt.axis('off')\n",
    "plt.savefig(os.path.join(res_dir, f'concatenate.jpg'), bbox_inches='tight')\n",
    "\n",
    "# then we deconcatenate:\n",
    "restored = Image.fromarray(out.astype(np.uint8)).resize((3*W, 3*H)) \\\n",
    "            .crop((W, H, 2*W, 2*H))\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(restored)\n",
    "plt.axis('off')\n",
    "plt.savefig(os.path.join(res_dir, f'deconcat.jpg'), bbox_inches='tight')\n",
    "\n",
    "del M, I, concat, out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dir = os.path.join(\"results\", \"prefilled\", \"staline\")\n",
    "os.makedirs(res_dir, exist_ok=True)\n",
    "\n",
    "M = load_mask(\"images/mask-staline.png\", output_shape=(600, 400))\n",
    "I = load_data(\"images/prefilled-staline.png\", output_shape=(600, 400))\n",
    "\n",
    "out = inpainting(model, datamean, I, M, postproc=False, skip=False, masking=False)\n",
    "vutils.save_image(out, os.path.join(res_dir, \"prefilled.jpg\"), normalize=True)\n",
    "out = (out.data.numpy()*255.).transpose((1,2,0)).astype(int)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(out)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "out = inpainting(model, datamean, I, M, postproc=False, skip=False, masking=True)\n",
    "vutils.save_image(out, os.path.join(res_dir, \"normal.jpg\"), normalize=True)\n",
    "out = (out.data.numpy()*255.).transpose((1,2,0)).astype(int)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(out)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "out = inpainting(model, datamean, I, M, postproc=False, skip=True, masking=True)\n",
    "vutils.save_image(out, os.path.join(res_dir, \"sanity-check.jpg\"), normalize=True)\n",
    "out = (out.data.numpy()*255.).transpose((1,2,0)).astype(int)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(out)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative prefilling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "res_dir = os.path.join(\"results\", \"prefilled\", \"iteratif\")\n",
    "os.makedirs(res_dir, exist_ok=True)\n",
    "\n",
    "M = load_mask(\"images/mask-staline.png\", output_shape=(600, 400))\n",
    "out = load_data(\"images/prefilled-staline.png\", output_shape=(600, 400))\n",
    "\n",
    "for i in range(4): \n",
    "    out = inpainting(model, datamean, out, M, postproc=False, skip=i==0, masking=i<=1)\n",
    "    vutils.save_image(out, os.path.join(res_dir, f\"iteration-{i}.jpg\"), normalize=True)\n",
    "    out_np = (out.data.numpy()*255.).transpose((1,2,0)).astype(int)\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.imshow(out_np)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "image = \"flower\"\n",
    "sizes = range(150,0,-20)\n",
    "n_sizes = len(sizes)\n",
    "res_dir = os.path.join(\"results\", \"fillin\", image)\n",
    "os.makedirs(res_dir, exist_ok=True)\n",
    "out = load_data(f\"images/{image}.jpg\", output_shape=(600, 400))\n",
    "\n",
    "def make_mask(tensor, size):\n",
    "    mask = torch.zeros_like(tensor)\n",
    "    ch, h, w = tensor.size()\n",
    "    py, px = h // 2, w // 2\n",
    "    mask[:, py-size:py+size, px-size:px+size] = 1.\n",
    "    return mask[:1, :, :]\n",
    "    \n",
    "    \n",
    "for i in range(n_sizes): \n",
    "    M = make_mask(out, sizes[i])\n",
    "\n",
    "    out = inpainting(model, datamean, out, M, postproc=False, skip=True, masking=True)\n",
    "    vutils.save_image(out, os.path.join(res_dir, f\"sanity-check-{i}.jpg\"), normalize=True)\n",
    "    out_np = (out.data.numpy()*255.).transpose((1,2,0)).astype(int)\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.imshow(out_np)\n",
    "    plt.axis('on')\n",
    "    plt.show()\n",
    "    \n",
    "    out = inpainting(model, datamean, out, M, postproc=False, skip=False, masking=True)\n",
    "    vutils.save_image(out, os.path.join(res_dir, f\"iteration-{i}.jpg\"), normalize=True)\n",
    "    out_np = (out.data.numpy()*255.).transpose((1,2,0)).astype(int)\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.imshow(out_np)\n",
    "    plt.axis('on')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks\n",
    "\n",
    "The local and global discriminators were not open-sourced. They are implemented in `models.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = _NetCompletion()\n",
    "summary(completion, input_size=(4, 512, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import _NetContext\n",
    "context = _NetContext()\n",
    "summary(context, [(3, 128, 128), (3, 256, 256)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "## Visualize mask and patch\n",
    "(Violet = global, green = local, yellow = hole)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.train import train_random_mask\n",
    "mask, patch = train_random_mask(2)\n",
    "m, p = mask[0,0], patch[0]\n",
    "m[p[0]:p[2], p[1]:p[3]] += 1\n",
    "plt.imshow(m.numpy())\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, context = get_networks(cuda=False)\n",
    "model, datamean = load_network()\n",
    "dataloader = load_dataset(dataset=\"cifar10\", dataroot=\"dataset/cifar10\", batch_size=2)\n",
    "train_discriminator(model, context, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Removing neurons in the pre-trained model\n",
    "Kind of an ablation study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = 0.1\n",
    "A = completionnet_places2\n",
    "A.load_state_dict(torch.load('completionnet_places2.pth'))\n",
    "B = completionnet_ablation(dropout)\n",
    "copy_weights(A, B)\n",
    "\n",
    "# activate dropout during eval\n",
    "B.eval()\n",
    "for m in B.modules():\n",
    "    if m.__class__.__name__.startswith('Dropout'):\n",
    "        m.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
