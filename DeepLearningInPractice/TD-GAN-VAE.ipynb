{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative modelling in deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generative modelling in machine learning can aim at achieving different goals.\n",
    "\n",
    "The first, obvious one is that a generative model can be used to generate more data, to be used afterwards by an other algorithm. While a generative model cannot create more information to solve the issue of having too small datasets, it could be used to solve anonymity questions. Typically, sharing a generative model trained on private data could allow the exploitation of the statistical property of this data without sharing the data itself (which can be protected by privacy matters for example).\n",
    "\n",
    "Another goal is to use generative modelling to better understand the data at hand. This is based on the hypothesis that a model that successfully learned to generate (and generalize) a dataset should have internally learned some efficient and compressed representation of the information contained in the data. In this case, analysing a posteriori the learned representation may give us insights on the data itself.\n",
    "\n",
    "The notion of a generative model however needs to be more formally specified, in order to work with. What does it mean for the model to generate data that \"looks like\" the original dataset? A mathematical formulation of that is necessary, in order to define a training objective that can be used efficiently. Having some expert rate the quality of all generated datapoints one by one is definitely not an option.\n",
    "\n",
    "Thus, modelling our data and models as probability distributions comes to the rescue. If we consider our data as coming from some underlying probability distribution, that we will name $p_D$, our goal is thus to train our model to represent another probability distribution, which we will name $p_\\theta$, that should be some good approximation of $p_D$. Given we only know $p_D$ through some set of realisations from it (the dataset), we can never hope to learn it exactly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1: Can you name some ways to compare two given distributions $p_D$ and $p_\\theta$?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Several measures are possible:\n",
    "> - Kullback-Leibler divergence is practical, but it is not symmetric;\n",
    "> - Jenson-Shannon is a symmetrized and smoothed version of the Kullback–Leibler divergence,\n",
    "> - Wasserstein distance is based on optimal transportation and is a smoother measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most comparison methods can be separated into two kinds: those that compare the density of the distributions ($p_\\theta(x)$ vs $p_D(x)$), and those that compare the values sampled from them directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2: Given we want to use them as an optimisation objective, what are the caveats to keep in mind about these two kinds?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Comparing the density of the distributions provide a general measure that does not reflect local differences.\n",
    "\n",
    "> Comparing the values sampled from the distributions might be too much computationnally expensive when many samples are drawn, or misleading in the other case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this work, we will focus on two of the most widely used generative models based on deep neural networks: Generative Adversarial Networks (GANs) and Variational AutoEncoders (VAEs), in order to compare them and understand their strenghts and weaknesses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative Adversarial Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GANs structure is based on modelling the distribution $p_\\theta$ as a learned deterministic function applied to a standard noise. Sampling from it is thus done as follows: first, some noise is sampled from a standard N-dimentional gaussian distribution: $\\epsilon \\sim \\mathcal{N}(0;I)$, and then the output is computed as a deterministic function $x = f_\\theta(\\epsilon)$. The function $f_\\theta$ is implemented as a neural network, $\\theta$ representing its learned parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3: What is, a priori, the impact of the choice of N, the dimension of the input noise?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If the dimension of N is too high, the model will be too hard to learn due to the curse of dimensionality.\n",
    "> If the dimension of N is too low, the model will not be learnable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By construction, this generator structure only allows sampling of the distribution $p_\\theta$, and does not allow the computation of the density $p_\\theta(x)$ (at least not without strong assumptions on $f_\\theta$. Such a model seems to need a comparison method based on samples to be trained.\n",
    "\n",
    "The smart idea of GANs is to instead use another neural network to model the objective. Another neural network is introduced: a classifier (that we call the discriminator) which is trained to differentiate examples from the dataset from examples generated by $p_\\theta$. The reasoning is as follows:\n",
    "\n",
    "The discriminator (whose output is denoted by $D(x)$) is trained using a classic discrimination loss, so that $D(x)$ can be interpreted as the probability that $x$ came from the real dataset:\n",
    "\n",
    "$$ \\mathcal{L}_D = \\mathbb{E}_{p_D} \\left[ -\\log D(x) \\right] + \\mathbb{E}_{p_\\theta} \\left[ -\\log \\left(1-D(x)\\right) \\right] $$\n",
    "\n",
    "From that, it can be shown that the optimal discriminator is given by $D(x) = \\frac{p_D(x)}{p_\\theta(x) + p_D(x)}$, and when reached its loss takes a specific value:\n",
    "\n",
    "$$ \\mathcal{L}_D = 2 \\left( \\log 2 - JSD(p_\\theta \\| p_D) \\right) $$\n",
    "\n",
    "So, training the generator network to *maximize* the same loss would, assuming the discriminator is always trained to optimality, minimize the JSD between $p_\\theta$ and $p_D$, and thus bring $p_\\theta$ closer to $p_D$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4: Can you anticipate a caveat of using the JSD as a training objective for the generator?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Using `log` function means the value of JSD has sudden jump, not differentiable everywhere. \n",
    "> This is an important source of unstability during training. Also, we might face a vanishing gradient as the JSD is null when the two distributors are too similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having the generator trained to maximize $\\mathcal{L}_D$ is equivalent to setting its training loss to $ \\mathcal{L}_G = \\mathbb{E}_{p_\\theta} \\log(1-D(x)) $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5: This loss only gives feedback to the generator on samples it generated, what can this imply?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> It means that the generator will never see real world data. Hence, if the data contains a lot of variety, the feedback provided by the discriminator might never contain such information. The discriminator might not be aware of variety. It is called `mode dropping`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now work on implementing a GAN on a simple toy problem, to get a feeling of its behavior and test our theoretical insights. For this we will use the `pytorch` library.\n",
    "\n",
    "While a real problem would be generating images for example (each datapoint $x$ would then be a different image), this is a kind of task that easily requires intensive CPU/GPU power, and image datasets are difficult to visualize from a geometric point of view (even small images contains hundreds of pixels, and nobody can visualize points in a 100-dimensional space). So instead we will focus on points in the plane: each datapoint $x$ will actually be a couple of numbers $(x1, x2)$, and our target dataset will be 25 Gaussian distributions with small variance, distributed on a $5\\times 5$ grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Our dataset is mathematically defined, we can generate batches on the fly and enjoy\n",
    "# an infinite-size dataset\n",
    "def generate_batch(batchlen):\n",
    "    \"\"\"This function generates a batch of length 'batchlen' from the 25-gaussian dataset.\n",
    "    \n",
    "    return a torch tensor of dimensions (batchlen, 2)\n",
    "    \"\"\"\n",
    "    # to sample from the gaussian mixture, we first sample the means for each point, then\n",
    "    # add a gaussian noise with small variance\n",
    "    samples = torch.multinomial(torch.tensor([0.2,0.2,0.2,0.2,0.2]), 2*batchlen, replacement=True)\n",
    "    means = (2.0 * (samples - 2.0)).view(batchlen,2).type(torch.FloatTensor)\n",
    "    return torch.normal(means, 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot a batch, to see what the dataset looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "batch = generate_batch(256)\n",
    "\n",
    "plt.scatter(batch[:,0], batch[:,1], s=2.0, label='Batch of data from our gaussian mixture dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to define our two neural networks, the generator and the discriminator. The generator will take as input a value $z$ sampled from a Gaussian prior, and output a value $x$ (thus a couple $(x_1,x_2)$). The discriminator takes as input a value $x$, and is a binary classifier.\n",
    "\n",
    "When representing a binary classifier with a neural network, it is better for the last layer to consist of only a sigmoid activation, so that the output values will be between 0 and 1 and stand for the probability (according to the classifier) that the input is of the first class. The output is thus of the form $\\mathrm{sigmoid}(h)$. The loss involves quantities such as $-\\log(\\mathrm{sigmoid}(h))$. For numerical stability reasons, it is recommended to rewrite the loss in order to make use of the $\\mathrm{softplus}$ function defined by $\\mathrm{softplus}(h) = \\log(1 + \\exp(h))$ and provided in PyTorch as `torch.softplus`.\n",
    "As the $\\mathrm{softplus}(h)$ formulation of the loss already contains the sigmoid activation, in practice the last layer of the discrimator network will not have any activation function (being just $h$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these classes in shape, only the training loop is still missing. To stick with the mathematical GAN framework, we should train the discriminator until convergence between each training step of the generator. This is not practical for two reasons: first it takes a lot of time, and second if the discriminator is too good, its gradient will vanish (as seen in **Q4**) and thus no information will be passed to the generator.\n",
    "\n",
    "We will then train the discriminator a fixed number of times between each training iteration of the generator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the previous code and train your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.latent_dim = latent_dim\n",
    "        def block(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.Linear(in_feat, out_feat)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *block(self.latent_dim, 128, normalize=False),\n",
    "            *block(128, 256),\n",
    "            *block(256, 512),\n",
    "            *block(512, 1024),\n",
    "            nn.Linear(1024, self.dim)\n",
    "            # nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def generate(self, batchlen):\n",
    "        z = torch.normal(torch.zeros(batchlen, PRIOR_N), 1.0)\n",
    "        return self.forward(z)\n",
    "\n",
    "    def forward(self, z):\n",
    "        img = self.model(z)\n",
    "        img = img.view(img.size(0), self.dim)\n",
    "        return img\n",
    "\n",
    "class LinearDiscriminator(nn.Module):\n",
    "    def __init__(self, latent_dim, dim):\n",
    "        super(LinearDiscriminator, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(self.dim, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        img_flat = img.view(img.size(0), -1)\n",
    "        validity = self.model(img_flat)\n",
    "        return validity\n",
    "\n",
    "\n",
    "\n",
    "class ConvDiscriminator(nn.Module):\n",
    "    def __init__(self, latent_dim, dim):\n",
    "        super(ConvDiscriminator, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(self.dim, 1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, 3, stride=2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(8, 8, 3, stride=2),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(392, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        img_flat = img.view(img.size(0), -1)\n",
    "        img_flat = self.fc1(img)\n",
    "        img = img_flat.view(img.size(0), 1, 32, 32)\n",
    "        img = self.conv1(img)\n",
    "        img_flat = img.view(img.size(0), -1)\n",
    "        validity = self.fc2(img_flat)\n",
    "        return validity\n",
    "    \n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "adversarial_loss = lambda x, y: (-y * torch.log(x) - (1-y)*torch.log(1-x) ).mean()\n",
    "latent_dim = 32\n",
    "dim = 2\n",
    "generator = Generator(latent_dim, dim)\n",
    "discriminator = ConvDiscriminator(latent_dim, dim)\n",
    "optimizer_G = torch.optim.Adam(generator.parameters())\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters())\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "if cuda:\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "    \n",
    "def train_gen(batch_size, n_train=1):\n",
    "    for _ in range(n_train):\n",
    "        optimizer_G.zero_grad()\n",
    "        z = Variable(Tensor(np.random.normal(0, 1, (batch_size, latent_dim))))\n",
    "        gen_batch = generator(z)\n",
    "        g_loss = adversarial_loss(discriminator(gen_batch), valid)\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "    return g_loss.item()\n",
    "\n",
    "def train_disc(batch_size, n_train=1):\n",
    "    for _ in range(n_train):\n",
    "        optimizer_D.zero_grad()\n",
    "        batch = generate_batch(batch_size)\n",
    "        real_batch = Variable(batch.type(Tensor))\n",
    "        z = Variable(Tensor(np.random.normal(0, 1, (batch_size, latent_dim))))\n",
    "        gen_batch = generator(z)\n",
    "        disc = discriminator(real_batch)\n",
    "        real_loss = adversarial_loss(discriminator(real_batch), valid)\n",
    "        fake_loss = adversarial_loss(discriminator(gen_batch.detach()), fake)\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "    return d_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start = 0\n",
    "n_epochs = 10000\n",
    "batch_size = 4096\n",
    "sample_interval = 500\n",
    "for epoch in range(start, n_epochs):\n",
    "    valid = Variable(Tensor(batch_size, 1).fill_(1.0), requires_grad=True)\n",
    "    fake = Variable(Tensor(batch_size, 1).fill_(0.0), requires_grad=True)\n",
    "\n",
    "    d_loss = train_disc(batch_size)\n",
    "    g_loss = train_gen(batch_size)\n",
    "    \n",
    "    if (epoch - 1) % (n_epochs // 30) == 0:\n",
    "        print (\"[Epoch %d/%d] [D loss: %f] [G loss: %f]\" % (epoch, n_epochs, d_loss, g_loss))\n",
    "\n",
    "    if (epoch - 1) % sample_interval == 0:\n",
    "        batch = generate_batch(batch_size)\n",
    "        real_batch = Variable(batch.type(Tensor)).cpu().data.numpy()\n",
    "        z = Variable(Tensor(np.random.normal(0, 1, (batch_size, latent_dim))))\n",
    "        gen_batch = generator(z).cpu().data.numpy()\n",
    "        plt.clf()\n",
    "        plt.scatter(real_batch[:,0], real_batch[:,1], s=2.0, label='real data')\n",
    "        plt.scatter(gen_batch[:,0], gen_batch[:,1], s=2.0, label='fake data', alpha=0.1)\n",
    "        plt.legend()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> I present a few results.:\n",
    "\n",
    "![First epoch](results/plots/results-1.png)\n",
    "\n",
    "> At the beginning, the generator generates points all at the center. The discriminator is not fooled, because real batch has a much bigger variance.\n",
    "\n",
    "![500 epoch](results/plots/results-501.png)\n",
    "> Therefore, the generator creates points with an higher variance. We can see that the generated points are following a certain curve. I guess this is due to the fact that the generator is linear and therefore it is a piecewise polynomial function.\n",
    "\n",
    "![3000 epoch](results/plots/results-3001.png)\n",
    "> Then, the generator finds the maximum values for all points\n",
    "\n",
    "![20000 epoch](results/plots/results-20001.png)\n",
    "> And slowly, it reduces the amount of points that have no integer values. We can observe that only a few points concentrate most of generated points. This is due to a collapsed mode. The batch length is of 4096 points. Therefore, I applied a transparency effect to highlight the high density of points around the initial distribution. \n",
    "\n",
    "> Unfortunately, the gan can not achieve much better results. I guess this is due to the discriminator model. I investigated a convolution layer, because the original distribution presents a translational invariance. \n",
    "\n",
    "\n",
    "\n",
    "Depending on your choice of parameters, the training may not go well at all, with the generator completely collapsing quickly at the beginning of the training. It has been observed by the litterature that the generator's loss $\\mathcal{L}_G = \\mathbb{E}_{p_\\theta} \\log(1-D(x))$ is often to blame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q6: Why could we anticipate that this loss could cause this?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> With this loss, the gradient vanishes when the two distributions are very far, and thus the generator is not able to learn anything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This issue is solved by replacing the generator loss by an alternative loss: $\\mathcal{L}_G = \\mathbb{E}_{p_\\theta} [ -\\log D(x) ]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q7: Inspect the impact of these different factors:**\n",
    "\n",
    "- depth / width of the generator network\n",
    "- depth / width of the discriminator network\n",
    "- impact of `TRAIN_RATIO`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> I observed that the discriminator tuning is more important than the generator. Certainly, the discriminator presented bottlenecks that kept the generator from converging.\n",
    "> I observed that the width of these networks is related to the number of latent variables. \n",
    "> I observed that the depth might keep the networks from converging. Instead, it appears more efficient to use a convolution layer, in order to reflect the invariances of $p_D$.\n",
    "> Increasing `TRAIN_RATIO` allows the discriminator to learn faster than the generator. If the discriminator learns too slowly, then the generator can fool it. If it learns too quickly, the generator is not able to learn anymore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For further readings on GANs, you can see the following papers:\n",
    "\n",
    "- Generative Adversarial Networks *(Goodfellow et al.)*: [arXiv:1406.2661](https://arxiv.org/abs/1406.2661)\n",
    "- Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks *(Radford et al.)*: [arXiv:1511.06434](https://arxiv.org/abs/1511.06434)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational AutoEncoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An other well-known approach to generative modelling is embodied by Variational AutoEncoders (VAEs). While the generative model itself and the procedure to sample it is similar to GANs, the way it is trained is not.\n",
    "\n",
    "The main goal of VAEs is to optimize the likelihood of the real data according to the generative model. In other words, maximize $\\mathbb{E}_{p_D} \\log p_\\theta(x)$, which is equivalent to minimizing $D_{KL}(p_D \\| p_\\theta)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q8: Prove this equivalence.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Maximizing the expectation term means that we maximize the empirical marginal likelihood:\n",
    "$\\sum_x p_D(x) \\log p_\\theta(x) = -D_{KL}(p_D \\| p_\\theta) + \\sum_x p_D(x) log p_D(x)$. The second term does not depend on $\\theta$. Therefore, maximizing $\\mathbb{E}_{p_D} \\log p_\\theta(x)$ is equivalent to minimizing $D_{KL}(p_D \\| p_\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the class of distributions for which $\\log p_\\theta(x)$ can be analytically computed and optimized is very restricted, and not suitable for real world problems. The main idea of the VAE is thus to introduce a latent variable $z$ and decompose the distribution like so: $p_\\theta(x, z) = p_\\theta(x | z) p(z)$. Where here $p(z)$ is some fixed prior and $p_\\theta(x | z)$ is a simple distribution whose parameters are the output of a neural network.\n",
    "\n",
    "For example, you could have $p(z)$ be a standard $\\mathcal{N}(0;1)$ and $p_\\theta(x | z)$ be defined as a gaussian $\\mathcal{N}(\\mu_\\theta(z); \\sigma_\\theta(z))$ where $\\mu_\\theta(z)$ and $\\sigma_\\theta(z)$ are created by the neural network you will train. In this case, the resulting distribution $p_\\theta(x) = \\int_z p_\\theta(x|z)p(z)ds$ is an infinite mixture of gaussians, which is a much more expressive class of distributions.\n",
    "\n",
    "Now, this cannot stop here, as we are not able to analitically compute the density $p_\\theta(x)$. The second main idea of the VAE is to introduce an other, auxilliary distribution: $q_\\phi(z | x)$, which will be modelled by a neural network similarly to $p_\\theta(x | z)$. Introducing it allows us to create a lower bound for $\\log p_\\theta(x)$:\n",
    "\n",
    "$$\\log p_\\theta(x) = \\mathbb{E}_{z \\sim q_\\phi} \\log p_\\theta(x) = \\mathbb{E}_{z \\sim q_\\phi} \\left[ \\log p_\\theta(x) \\frac {q_\\phi(z|x)}{q_\\phi(z|x)} \\right]$$\n",
    "\n",
    "Following Bayes theorem, $p_\\theta(x) p_\\theta(z|x) = p_\\theta(x, z) = p_\\theta(x|z) p(z)$, so we get:\n",
    "\n",
    "$$\\log p_\\theta(x) = \\mathbb{E}_{z \\sim q_\\phi} \\left[ \\log \\frac{p_\\theta(x|z) p(z)}{p_\\theta(z|x)} \\frac {q_\\phi(z|x)}{q_\\phi(z|x)} \\right]$$\n",
    "\n",
    "Re-organizing the terms:\n",
    "\n",
    "$$\\log p_\\theta(x) = \\mathbb{E}_{z \\sim q_\\phi} \\log \\frac{q_\\phi(z|x)}{p_\\theta(z|x)} - \\mathbb{E}_{z \\sim q_\\phi} \\log \\frac{p(z)}{q_\\phi(z|x)} + \\mathbb{E}_{z \\sim q_\\phi} \\log p_\\theta(x | z)$$\n",
    "\n",
    "This can be re-expressed like so:\n",
    "\n",
    "$$\\log p_\\theta(x) = D_{KL}(q_\\phi(z | x) \\| p_\\theta(z | x)) - D_{KL}(q_\\phi(z | x) \\| p(z)) + \\mathbb{E}_{z \\sim q_\\phi} \\log p_\\theta(x|z)$$\n",
    "\n",
    "The 3 terms of this equality can be interpreted like so:\n",
    "\n",
    "- the first term measures how much $q_\\phi(z | x)$ is similar to $p_\\theta(z | x)$, or in other words is a good inverse of $p_\\theta(x | z)$\n",
    "- the second term measures how similar $q_\\phi(z|x)$ is from the latent prior $p(z)$\n",
    "- the third term is linked to how likely $p_\\theta$ is to yield the given $x$ when $z$ is sampled from $q_\\phi(z | x)$ rather than $p(z)$\n",
    "\n",
    "It is interesting to note that the first term, being a KL-divergence is always positive. As such the combination of the last two terms form a lower bound of $\\log p_\\theta(x)$ which *can* be computed and used as a training objective. This bound is called the *Evidence Lower-Bound (ELBO)*. Simply flipping its sign can make it into a loss that can be minimized by gradient descent:\n",
    "\n",
    "$$ \\mathcal{L}_{ELBO} = D_{KL}(q_\\phi(z | x) \\| p(z)) + \\mathbb{E}_{z \\sim q_\\phi} [ - \\log p_\\theta(x|z) ]$$\n",
    "\n",
    "From this formulation comes the parallel with auto-encoders that give the VAE its name: $q_\\phi(z | x)$ can be seen as a *probabilistic encoder* from the data $x$ to the latent space $z$, and $p_\\theta(x | z)$ can be seen as a *probabilistic decoder* from the latent space $z$ to the data $x$. In this case the second term of $\\mathcal{L}_{ELBO}$ is the loss measuring the reconstruction quality of the auto-encoder, and the first term can be seens as a regularization of the latent space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q9: We can see that $p(z)$ is never sampled during the training process, how can that be a problem?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Because $p(z)$ is never employed during the training process, we can not be sure its representation is correct or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical choice to represent $q_\\phi(z | x)$ is to use a diagonal Gaussian distribution $\\mathcal{N}(\\mu_\\phi(x); Diag(\\sigma_\\phi(x)))$, which makes the KL-divergence term of $\\mathcal{L}_{ELBO}$ analytically computable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q10: Assuming $p(z)$ is a $\\mathcal{N}(0; Id)$ gaussian, what is the value of $D_{KL}(q_\\phi(z | x) \\| p(z))$?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $\\frac{1}{2}[(\\sum_{i=1}^{n}\\mu_{i}^{2} + \\sum_{i=1}^{n}\\sigma_{\\phi_i}^{2}) - \\sum_{i=1}^{n}(log(\\sigma_{\\phi_i}^{2}) + 1)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also model $p_\\theta(x | z)$ as a diagonal Gaussian $\\mathcal{N}(\\mu_\\theta(z); Diag(\\sigma_\\theta(z)))$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q11: What is the expression of $-\\log p_\\theta(x | z)$ for given $x$ and $z$?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $\\frac{1}{2} \\sum_{i=1}^n \\log{2\\pi \\sigma_{\\theta_i}^2} + \\frac{1}{2} \\left(x-\\mu_\\theta(z)\\right)^T Diag(\\sigma_\\theta(z))\\left( x - \\mu_\\theta(z)\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build and train a VAE using the same dataset as previously, in order to compare its behavior to GANs. For numerical stability, we will interpret the output of the encoder and decoder networks as $(\\mu, \\log\\sigma^2)$, rather than $(\\mu, \\sigma)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn \n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_batch(batchlen):\n",
    "    \"\"\"This function generates a batch of length 'batchlen' from the 25-gaussian dataset.\n",
    "\n",
    "    return a torch tensor of dimensions (batchlen, 2)\n",
    "    \"\"\"\n",
    "    # to sample from the gaussian mixture, we first sample the means for each point, then\n",
    "    # add a gaussian noise with small variance\n",
    "    samples = torch.multinomial(torch.tensor([0.2,0.2,0.2,0.2,0.2]), 2*batchlen, replacement=True)\n",
    "    means = (2.0 * (samples - 2.0)).view(batchlen,2).type(torch.FloatTensor)\n",
    "    return torch.normal(means, 0.05)\n",
    "\n",
    "\n",
    "# Choose a value for the latent dimension\n",
    "LATENT_N = 3\n",
    "\n",
    "# Define the generator\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(2, 2)\n",
    "        self.fc_mu = nn.Linear(2, LATENT_N)\n",
    "        self.fc_logvar = nn.Linear(2, LATENT_N)\n",
    "        \n",
    "    # encode a datapoint. This should return a couple of tensors (mu, logvar) representing\n",
    "    # the parameters of the gaussian q_\\phi(z | x)\n",
    "    def __call__(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return (mu, logvar)\n",
    "    \n",
    "\n",
    "# Define the discriminator\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(LATENT_N, 2)\n",
    "        self.fc_mu = nn.Linear(2, 2)\n",
    "        self.fc_logvar = nn.Linear(2, 2)\n",
    "    \n",
    "    # decode a datapoint. This should return a couple of tensors (mu, logvar) representing\n",
    "    # the parameters of the gaussian p_\\theta(z | x)\n",
    "    def __call__(self, z):\n",
    "        h = F.elu(self.fc1(z))\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return (mu, logvar)\n",
    "\n",
    "    def generate(self, batchlen):\n",
    "        z = torch.normal(torch.zeros(batchlen, LATENT_N), 1.0)\n",
    "        (mu, logvar) = self.__call__(z)\n",
    "        return torch.normal(mu, torch.exp(0.5*logvar))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, the parameters of both networks are trained conjointly using the same loss $\\mathcal{L}_{ELBO}$. Pytorch allows us to sample the Gaussian distribution in a differentiable way using `torch.normal(mu, sigma)`, but it is not differentiable wrt to its inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q12: How can you sample a distribution $\\mathcal{N}(\\mu, \\sigma)$ is a way that is differentiable w.r.t. both $\\mu$ and $\\sigma$?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> According to the original paper (Section 2.4), $z = \\mu + \\sigma \\epsilon$, where $\\epsilon ∼ \\mathcal N (0, 1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Total number of training iterations for the VAE\n",
    "N_ITER = 40001\n",
    "# Batch size to use\n",
    "BATCHLEN = 10\n",
    "\n",
    "encoder = Encoder()\n",
    "optim_enc = torch.optim.Adam(encoder.parameters(), lr=0.001, betas=(0.5,0.9))\n",
    "decoder = Decoder()\n",
    "optim_dec = torch.optim.Adam(decoder.parameters(), lr=0.001, betas=(0.5,0.9))\n",
    "mse_loss = torch.nn.MSELoss(size_average=False)\n",
    "\n",
    "for i in range(N_ITER):\n",
    "    encoder.zero_grad()\n",
    "    decoder.zero_grad()\n",
    "    \n",
    "    x = generate_batch(BATCHLEN)\n",
    "    \n",
    "    enc_mu, enc_logvar = encoder(x)\n",
    "    # Compute here the DKL part of the VAE loss\n",
    "    loss_kl = 0#torch.mean(.5 * torch.sum((enc_mu**2) + torch.exp(enc_logvar) - 1 - enc_logvar, 1))\n",
    "\n",
    "    # Compute here the sample z, using Q12\n",
    "    std = torch.exp(0.5*enc_logvar)\n",
    "    eps = torch.randn_like(std)\n",
    "    z = eps.mul(std).add_(enc_mu)\n",
    "    dec_mu, dec_logvar = decoder(z)\n",
    "    \n",
    "    # Compute here the second part of the VAE loss\n",
    "    x_r = torch.normal(dec_mu, torch.exp(0.5*dec_logvar))\n",
    "    loss_rec = mse_loss(x_r, x) / BATCHLEN\n",
    "\n",
    "    (loss_kl + loss_rec).backward()\n",
    "    optim_enc.step()\n",
    "    optim_dec.step()\n",
    "    if i%1000 == 0:\n",
    "        #print(enc_mu, enc_logvar, dec_mu, dec_logvar)\n",
    "        print('step {}: KL: {:.3e}, rec: {:.3e}'.format(i, float(loss_kl), float(loss_rec)))\n",
    "        # plot the result\n",
    "        real_batch = generate_batch(1024)\n",
    "        rec_batch = torch.normal(dec_mu, torch.exp(0.5*dec_logvar)).detach()\n",
    "        fake_batch = decoder.generate(1024).detach()\n",
    "        plt.scatter(real_batch[:,0], real_batch[:,1], s=2.0, label='real data')\n",
    "        plt.scatter(rec_batch[:,0], rec_batch[:,1], s=2.0, label='rec data')\n",
    "        plt.scatter(fake_batch[:,0], fake_batch[:,1], s=2.0, label='fake data')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q13: Try hardcoding $\\sigma_\\theta(z)$ to some small value (like 0.01) rather than allowing the decoder to learn it. What does it change?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (write your answer here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q14: How do the power of encoder and decoder affect the overall training of the VAE?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (write your answer here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q15: As a conclusion, how would you compare the advantages and shortcomings of GANs and VAEs?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (write your answer here)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
